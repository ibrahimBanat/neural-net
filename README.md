# Neural Networks Fundamentals 
<h1> SoftMax.py</h1>
<p> Softmax function, which is the equivalent of the sigmoid activation function, but when the problem has 3 or more classes.
 Softmax is used in neural networks, to map the non-normalized output of a network to a probability distribution over predicted output classes.</p>
 <br>
 <h1> cross-entropy.py</h1>
 <p>Cross-entropy measures the relative entropy between two probability distributions over the same set of events.
 Intuitively, to calculate cross-entropy between P and Q, you simply calculate entropy for Q using probability weights from P.</p>
<br> 
<h1> Gradient Descent</h1>
<ul><li><a href="Gradiant-Descent/README.md">README</a></ul>


