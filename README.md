Neural Networks fundamentals 
<h1> SoftMax.py</h1>
<p> Softmax function, which is the equivalent of the sigmoid activation function, but when the problem has 3 or more classes.
 Softmax is used in neural networks, to map the non-normalized output of a network to a probability distribution over predicted output classes.</p>
 
